"""
Huáº¥n luyá»‡n model AI dá»± Ä‘oÃ¡n nguy cÆ¡ nghá»‰ viá»‡c
Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ tá»« process_data.py
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import os

def load_processed_data():
    """Äá»c dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½"""
    data_path = 'data/nhan_su_processed.csv'
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file {data_path}. HÃ£y cháº¡y process_data.py trÆ°á»›c!")
    
    df = pd.read_csv(data_path, index_col=0)
    print(f"âœ… ÄÃ£ Ä‘á»c {len(df)} nhÃ¢n viÃªn vá»›i {len(df.columns)} features")
    
    return df

def prepare_features_and_labels(df):
    """Chuáº©n bá»‹ features vÃ  labels cho model"""
    # Danh sÃ¡ch features (bá» qua cá»™t label náº¿u cÃ³)
    feature_columns = [
        'so_ngay_di_lam',
        'so_lan_di_muon', 
        'so_lan_ve_som',
        'gio_lam_viec_tb',
        'tong_ngay_nghi_phep',
        'so_don_nghi_phep',
        'so_nam_lam_viec',
        'phong_ban_encoded',
        'chuc_vu_encoded',
        'tuoi'
    ]
    
    # Kiá»ƒm tra xem cÃ³ Ä‘á»§ features khÃ´ng
    missing_features = [col for col in feature_columns if col not in df.columns]
    if missing_features:
        print(f"âš ï¸ Thiáº¿u features: {missing_features}")
        # Chá»‰ sá»­ dá»¥ng features cÃ³ sáºµn
        feature_columns = [col for col in feature_columns if col in df.columns]
    
    X = df[feature_columns].copy()
    
    # Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u
    X = X.fillna(0)
    
    # Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    # Chuáº©n bá»‹ labels
    if 'nghi_viec' in df.columns:
        y = df['nghi_viec']
        print(f"âœ… Sá»­ dá»¥ng label 'nghi_viec' cÃ³ sáºµn")
    else:
        # Táº¡o label giáº£ Ä‘á»‹nh dá»±a trÃªn cÃ¡c tiÃªu chÃ­
        print("âš ï¸ KhÃ´ng cÃ³ label 'nghi_viec', táº¡o label giáº£ Ä‘á»‹nh...")
        y = create_synthetic_labels(df)
    
    print(f"ğŸ“Š PhÃ¢n bá»‘ labels: {y.value_counts().to_dict()}")
    
    return X_scaled, y, scaler, feature_columns

def create_synthetic_labels(df):
    """Táº¡o labels giáº£ Ä‘á»‹nh cho má»¥c Ä‘Ã­ch demo"""
    # Táº¡o label dá»±a trÃªn cÃ¡c tiÃªu chÃ­:
    # - Nghá»‰ phÃ©p nhiá»u (>5 ngÃ y/nÄƒm)
    # - Äi muá»™n nhiá»u (>5 láº§n/nÄƒm) 
    # - Vá» sá»›m nhiá»u (>3 láº§n/nÄƒm)
    # - Ãt ngÃ y Ä‘i lÃ m (<10 ngÃ y/nÄƒm)
    
    # Táº¡o Ä‘iá»u kiá»‡n tá»•ng há»£p
    high_risk = (
        (df['tong_ngay_nghi_phep'] > 5) |
        (df['so_lan_di_muon'] > 5) |
        (df['so_lan_ve_som'] > 3) |
        (df['so_ngay_di_lam'] < 10)
    )
    
    y = np.where(high_risk, 1, 0)  # 1: cÃ³ nguy cÆ¡ nghá»‰ viá»‡c, 0: khÃ´ng
    
    # Äáº£m báº£o cÃ³ Ã­t nháº¥t 2 class
    if y.sum() == 0:
        # Náº¿u khÃ´ng cÃ³ ai cÃ³ nguy cÆ¡, chá»n ngáº«u nhiÃªn 20% nhÃ¢n viÃªn
        n_high_risk = max(1, int(len(df) * 0.2))
        high_risk_indices = np.random.choice(len(df), n_high_risk, replace=False)
        y[high_risk_indices] = 1
    elif y.sum() == len(df):
        # Náº¿u táº¥t cáº£ Ä‘á»u cÃ³ nguy cÆ¡, chá»n ngáº«u nhiÃªn 80% nhÃ¢n viÃªn
        n_low_risk = max(1, int(len(df) * 0.8))
        low_risk_indices = np.random.choice(len(df), n_low_risk, replace=False)
        y[low_risk_indices] = 0
    
    return pd.Series(y, index=df.index)

def train_model(X, y):
    """Huáº¥n luyá»‡n model Random Forest"""
    print("ğŸ¤– Báº¯t Ä‘áº§u huáº¥n luyá»‡n model...")
    
    # Chia dá»¯ liá»‡u train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"ğŸ“ˆ Train set: {len(X_train)} samples")
    print(f"ğŸ“Š Test set: {len(X_test)} samples")
    
    # Huáº¥n luyá»‡n model
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        class_weight='balanced'  # Xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u
    )
    
    model.fit(X_train, y_train)
    
    # ÄÃ¡nh giÃ¡ model
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # In káº¿t quáº£ Ä‘Ã¡nh giÃ¡
    print("\nğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ model:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    print(f"\nCross-validation scores: {cv_scores}")
    print(f"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    
    return model, X_test, y_test, y_pred, y_pred_proba

def analyze_feature_importance(model, feature_names):
    """PhÃ¢n tÃ­ch táº§m quan trá»ng cá»§a cÃ¡c features"""
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ¯ Táº§m quan trá»ng cá»§a features:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"  {row['feature']}: {row['importance']:.3f}")
    
    return feature_importance

def save_model_and_metadata(model, scaler, feature_names, model_info):
    """LÆ°u model vÃ  metadata"""
    # Táº¡o thÆ° má»¥c models náº¿u chÆ°a cÃ³
    models_dir = 'models'
    os.makedirs(models_dir, exist_ok=True)
    
    # LÆ°u model
    model_path = f'{models_dir}/attrition_model.pkl'
    joblib.dump(model, model_path)
    
    # LÆ°u scaler
    scaler_path = f'{models_dir}/scaler.pkl'
    joblib.dump(scaler, scaler_path)
    
    # LÆ°u metadata
    metadata = {
        'feature_names': feature_names,
        'model_info': model_info,
        'model_path': model_path,
        'scaler_path': scaler_path
    }
    
    metadata_path = f'{models_dir}/model_metadata.json'
    import json
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ÄÃ£ lÆ°u model vÃ o: {model_path}")
    print(f"âœ… ÄÃ£ lÆ°u scaler vÃ o: {scaler_path}")
    print(f"âœ… ÄÃ£ lÆ°u metadata vÃ o: {metadata_path}")

def main():
    """HÃ m chÃ­nh huáº¥n luyá»‡n model"""
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n model AI...")
    
    try:
        # Äá»c dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        df = load_processed_data()
        
        # Chuáº©n bá»‹ features vÃ  labels
        X, y, scaler, feature_names = prepare_features_and_labels(df)
        
        # Huáº¥n luyá»‡n model
        model, X_test, y_test, y_pred, y_pred_proba = train_model(X, y)
        
        # PhÃ¢n tÃ­ch feature importance
        feature_importance = analyze_feature_importance(model, feature_names)
        
        # LÆ°u model vÃ  metadata
        model_info = {
            'algorithm': 'RandomForestClassifier',
            'n_estimators': 100,
            'max_depth': 10,
            'accuracy': accuracy_score(y_test, y_pred),
            'cv_accuracy': cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()
        }
        
        save_model_and_metadata(model, scaler, feature_names, model_info)
        
        print("\nğŸ‰ HoÃ n thÃ nh huáº¥n luyá»‡n model!")
        print("ğŸ’¡ BÆ°á»›c tiáº¿p theo: Cháº¡y predict_api.py Ä‘á»ƒ táº¡o API dá»± Ä‘oÃ¡n")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o Ä‘Ã£ cháº¡y process_data.py trÆ°á»›c!")

if __name__ == "__main__":
    main() 